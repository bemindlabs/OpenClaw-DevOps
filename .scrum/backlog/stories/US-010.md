# US-010: LLM Provider Abstraction Layer

## Story Details

| Field | Value |
|-------|-------|
| **Story ID** | US-010 |
| **Epic** | EPIC-GATEWAY |
| **Title** | LLM Provider Abstraction Layer |
| **Priority** | Critical |
| **Story Points** | 8 |
| **Status** | Ready |
| **Assignee** | Unassigned |
| **Sprint** | Sprint 1 |

## User Story

**As a** developer integrating with OpenClaw,
**I want** a unified API for multiple LLM providers,
**So that** I can switch providers without changing my application code.

## Description

Implement an abstraction layer that normalizes requests and responses across multiple LLM providers (OpenAI, Anthropic, Google AI, Mistral, Groq, OpenRouter). The layer should provide a consistent interface while handling provider-specific authentication, request formatting, and response parsing.

## Acceptance Criteria

- [ ] Support for 6+ LLM providers implemented
- [ ] Unified request format (OpenAI-compatible)
- [ ] Unified response format across all providers
- [ ] Provider-specific authentication handled internally
- [ ] Model mapping (e.g., "gpt-4" -> provider-specific model)
- [ ] Error normalization across providers
- [ ] Token counting for all providers
- [ ] Streaming support for all providers
- [ ] Configuration via environment variables
- [ ] Provider health check endpoints

## Technical Notes

### IEEE Requirement References
- IEEE-STD-GW-001: Multi-Provider Support

### Provider Interface

```typescript
// services/providers/base.ts
export interface LLMProvider {
  name: string;
  models: string[];

  chat(request: ChatRequest): Promise<ChatResponse>;
  chatStream(request: ChatRequest): AsyncGenerator<ChatChunk>;
  embeddings(request: EmbeddingRequest): Promise<EmbeddingResponse>;

  countTokens(text: string, model: string): number;
  healthCheck(): Promise<boolean>;
}

export interface ChatRequest {
  model: string;
  messages: Message[];
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
}

export interface ChatResponse {
  id: string;
  model: string;
  choices: Choice[];
  usage: Usage;
}
```

### Provider Implementations

```typescript
// services/providers/openai.ts
export class OpenAIProvider implements LLMProvider {
  name = "openai";
  models = ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"];

  private client: OpenAI;

  constructor() {
    this.client = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }

  async chat(request: ChatRequest): Promise<ChatResponse> {
    const response = await this.client.chat.completions.create({
      model: request.model,
      messages: request.messages,
      temperature: request.temperature,
      max_tokens: request.max_tokens,
    });
    return this.normalizeResponse(response);
  }
}

// services/providers/anthropic.ts
export class AnthropicProvider implements LLMProvider {
  name = "anthropic";
  models = ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"];

  async chat(request: ChatRequest): Promise<ChatResponse> {
    // Convert OpenAI format to Anthropic format
    const anthropicRequest = this.convertRequest(request);
    const response = await this.client.messages.create(anthropicRequest);
    return this.normalizeResponse(response);
  }
}
```

### Provider Registry

```typescript
// services/providers/registry.ts
export class ProviderRegistry {
  private providers: Map<string, LLMProvider> = new Map();

  register(provider: LLMProvider): void {
    this.providers.set(provider.name, provider);
  }

  get(name: string): LLMProvider | undefined {
    return this.providers.get(name);
  }

  getForModel(model: string): LLMProvider | undefined {
    for (const provider of this.providers.values()) {
      if (provider.models.includes(model)) {
        return provider;
      }
    }
    return undefined;
  }

  list(): string[] {
    return Array.from(this.providers.keys());
  }
}
```

### Model Mapping

```typescript
// config/model-mapping.ts
export const MODEL_MAPPING: Record<string, { provider: string; model: string }> = {
  "gpt-4": { provider: "openai", model: "gpt-4" },
  "gpt-4-turbo": { provider: "openai", model: "gpt-4-turbo-preview" },
  "claude-3": { provider: "anthropic", model: "claude-3-sonnet-20240229" },
  "gemini-pro": { provider: "google", model: "gemini-pro" },
};
```

### Files to Create/Modify
- `apps/gateway/services/providers/base.ts`
- `apps/gateway/services/providers/openai.ts`
- `apps/gateway/services/providers/anthropic.ts`
- `apps/gateway/services/providers/google.ts`
- `apps/gateway/services/providers/mistral.ts`
- `apps/gateway/services/providers/groq.ts`
- `apps/gateway/services/providers/openrouter.ts`
- `apps/gateway/services/providers/registry.ts`
- `apps/gateway/config/model-mapping.ts`

## Dependencies

- Provider SDKs installed (openai, @anthropic-ai/sdk, etc.)
- API keys configured in environment

## Definition of Done

- [ ] All 6 providers implemented
- [ ] Unit tests for each provider
- [ ] Integration tests with real API calls
- [ ] Request/response normalization verified
- [ ] Token counting accurate
- [ ] Streaming working for all providers
- [ ] Documentation for adding new providers

---

*Created: 2026-02-08*
